{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d88d3d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# import libraries needed\n",
    "from seebuoy import NDBC\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm, skew, probplot\n",
    "from scipy.special import boxcox1p\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from datetime import timedelta, datetime\n",
    "from prophet_rse import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef69bbc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def gather_data(buoy_num):\n",
    "    \"\"\"\n",
    "    Imports data from the National Data Buoy Center (NOAA)\n",
    "    @param buoy_num: the buoy number of the closest x,y coordinates\n",
    "    @return see_buoy: the buoy data in a dataframe\n",
    "    \"\"\"\n",
    "    ndbc = NDBC()\n",
    "    \n",
    "    # Information on NDBC's ~1800 buoys and gliders\n",
    "    wave_df = ndbc.stations()\n",
    "\n",
    "    # list all available data for all buoys\n",
    "    df_data = ndbc.available_data()\n",
    "\n",
    "    # Get info on a single buoy\n",
    "    see_buoy = ndbc.get_data(buoy_num)\n",
    "\n",
    "    return see_buoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a772e08",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def handle_missing_data(buoy):\n",
    "    \"\"\"\n",
    "    The data has some missing values. We impute these values with interpolation.\n",
    "\n",
    "    @param buoy: the data imported from Seebuoy\n",
    "    @return buoy_interpolated: a dataframe of buoy data where we impute the missing values\n",
    "    with interpolation\n",
    "    \"\"\"\n",
    "    # missing data\n",
    "    total = buoy.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (buoy.isnull().sum() / buoy.isnull().count()).sort_values(\n",
    "        ascending=False\n",
    "    )\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=[\"Total\", \"Percent\"])\n",
    "\n",
    "    # dropping rows where average_period is null\n",
    "    buoy.dropna(subset=['average_period'], inplace=True)\n",
    "\n",
    "    # dropping rows wehre wave_height is null\n",
    "    buoy.dropna(subset=['wave_height'], inplace=True)\n",
    "\n",
    "    # dropping cols where there is 100% NA\n",
    "    buoy.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # IMPUTATIONS\n",
    "\n",
    "    columns_to_fill = [\n",
    "        \"wind_direction\",\n",
    "        \"wind_speed\",\n",
    "        \"wind_gust\",\n",
    "        \"dominant_period\",\n",
    "        \"mean_wave_direction\",\n",
    "        \"pressure\",\n",
    "        \"air_temp\",\n",
    "        \"water_temp\",\n",
    "        \"dewpoint\",\n",
    "        \"visibility\",\n",
    "        \"pressure_tendency\",\n",
    "        \"tide\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Interpolate missing values using spline interpolation\n",
    "    buoy_interpolated = buoy.copy()\n",
    "    for column in columns_to_fill:\n",
    "        if column in buoy_interpolated:\n",
    "            buoy_interpolated[column] = buoy_interpolated[column].fillna(buoy_interpolated[column].interpolate(method='spline', order=2))\n",
    "    \n",
    "    # check if there are any additional missing values\n",
    "    for column in columns_to_fill:\n",
    "        if column in buoy_interpolated.columns and buoy_interpolated[column].isnull().any():\n",
    "            buoy_interpolated = buoy_interpolated.drop(column, axis=1)\n",
    "\n",
    "    return buoy_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ac445",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def time_series_split_regression(\n",
    "    data,\n",
    "    regressor,\n",
    "    date_column=\"date\",\n",
    "    target_column=\"wave_height\",\n",
    "    cols_to_ignore=[],\n",
    "    n_splits=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform time series split on a pandas DataFrame based on a date column and\n",
    "    train a regression model, calculating RMSE for each split.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame\n",
    "    - regressor: scikit-learn regressor object\n",
    "        The regression algorithm to use.\n",
    "    - date_column: str, default=\"date\"\n",
    "        The name of the date column in the DataFrame.\n",
    "    - target_column: str default=\"wave_height\"\n",
    "        The name of the target column in the DataFrame.\n",
    "    - n_splits: int, default=5\n",
    "        Number of splits for TimeSeriesSplit.\n",
    "    - tune_hyperparameters: bool, default=False\n",
    "\n",
    "    Returns:\n",
    "    - result_df: pandas DataFrame\n",
    "        DataFrame containing the Id, actual value, predicted value, fold, and whether it was in the test or train set.\n",
    "    - rmse_scores: list of floats\n",
    "        List of RMSE scores for each split.\n",
    "    - split_dates: list of tuples\n",
    "        List of (min_date, max_date) tuples for each split.\n",
    "    - num_records: list of tuples\n",
    "        List of (train_size, test_size) tuples for each split.\n",
    "    - two_week_predictions: dataframe containing the predictions for the target two weeks into the future.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the DataFrame based on the date column\n",
    "    data = data.sort_values(by=date_column)\n",
    "    # data = data.set_index('date')\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    # Time Series Split training\n",
    "    rmse_scores = []\n",
    "    split_dates = []\n",
    "    num_records = []\n",
    "    all_predictions = []\n",
    "\n",
    "    regressor = train_time_series(tscv, data, target_column, date_column, split_dates, num_records, all_predictions, rmse_scores, regressor, cols_to_ignore)\n",
    "    \n",
    "    # Make prediction for the future\n",
    "    # Extend time horizon for predictions (two weeks from now)\n",
    "    start_date = datetime.now()\n",
    "    end_date = start_date + timedelta(days=14)  # Extend two weeks\n",
    "    prediction_dates = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    \n",
    "    # Create a DataFrame from prediction dates\n",
    "    prediction_dates_df = pd.DataFrame(prediction_dates, columns=[\"date\"])\n",
    "\n",
    "    # Add additional columns with empty values\n",
    "    # prediction_dates_df[\"wind_speed\"] = pd.NA\n",
    "    # prediction_dates_df[\"wind_gust\"] = pd.NA\n",
    "    # prediction_dates_df[\"mean_wave_direction\"] = pd.NA\n",
    "    # prediction_dates_df[\"water_temp\"] = pd.NA\n",
    "    \n",
    "    prediction_dates_df[\"wind_direction\"] = pd.NA\n",
    "    prediction_dates_df[\"wind_speed\"] = pd.NA\n",
    "    prediction_dates_df[\"wind_gust\"] = pd.NA\n",
    "    prediction_dates_df[\"dominant_period\"] = pd.NA\n",
    "    prediction_dates_df[\"mean_wave_direction\"] = pd.NA\n",
    "    prediction_dates_df[\"pressure\"] = pd.NA\n",
    "    prediction_dates_df[\"air_temp\"] = pd.NA\n",
    "    prediction_dates_df[\"water_temp\"] = pd.NA\n",
    "    prediction_dates_df[\"dewpoint\"] = pd.NA\n",
    "    prediction_dates_df[\"visibility\"] = pd.NA\n",
    "    prediction_dates_df[\"pressure_tendency\"] = pd.NA\n",
    "    prediction_dates_df[\"tide\"] = pd.NA\n",
    "\n",
    "    if target_column == \"wave_height\":\n",
    "        prediction_dates_df[\"average_period\"] = pd.NA\n",
    "    elif target_column == \"average_period\":\n",
    "        prediction_dates_df[\"wave_height\"] = pd.NA\n",
    "\n",
    "    # make a new DataFrame with columns from old data\n",
    "    extended_data = data.iloc[:, :]\n",
    "    extended_data = pd.concat([extended_data, prediction_dates_df], axis=0)\n",
    "\n",
    "    buoy_interpolated = handle_missing_data(extended_data)\n",
    "    \n",
    "    buoy_interpolated = buoy_interpolated.drop([\"date\", target_column], axis=1)\n",
    "    two_week_predictions = regressor.predict(buoy_interpolated)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    result_df = pd.DataFrame(\n",
    "        all_predictions, columns=[\"date\", \"Actual\", \"Predicted\", \"Fold\", \"Set\"]\n",
    "    )\n",
    "\n",
    "    return result_df, rmse_scores, split_dates, num_records, two_week_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time_series(tscv, data, target_column, date_column, split_dates, num_records, all_predictions, rmse_scores, regressor, cols_to_ignore):\n",
    "    \"\"\"\n",
    "    Do the folds and training for each split\n",
    "\n",
    "    Parameters:\n",
    "    - tscv: Time series split object\n",
    "    - data: pandas DataFrame\n",
    "    - regressor: scikit-learn regressor object\n",
    "        The regression algorithm to use.\n",
    "    - date_column: str, default=\"date\"\n",
    "        The name of the date column in the DataFrame.\n",
    "    - target_column: str, default=\"wave_height\"\n",
    "        The name of the target column in the DataFrame.\n",
    "    - split_dates: list, empty list to be used in folds\n",
    "    - num_records: list, empty list to be used to show the size of each train test split\n",
    "    - all_predictions: list, empty list of predictions to be appended onto \n",
    "    - rmse_scores: list, empty list of rmse scores to be calculated and appended for each test\n",
    "    - regressor: the regression algorithm to use\n",
    "    - cols_to_ignore: list, columns to drop before training\n",
    "\n",
    "    Returns:\n",
    "    - regressor: the trained model\n",
    "\n",
    "    \"\"\"\n",
    "    # Perform the time series split and train regression model for each split\n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(data)):\n",
    "        train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "        cols_to_ignore = cols_to_ignore + [target_column, date_column]\n",
    "\n",
    "        X_train = train_data.drop(cols_to_ignore, axis=1)\n",
    "        X_test = test_data.drop(cols_to_ignore, axis=1)\n",
    "        y_train, y_test = train_data[target_column], test_data[target_column]\n",
    "\n",
    "        # Record the minimum and maximum dates for each split\n",
    "        min_date, max_date = test_data[date_column].min(), test_data[date_column].max()\n",
    "        split_dates.append((min_date, max_date))\n",
    "\n",
    "        # Train regression model\n",
    "        regressor.fit(\n",
    "            X_train, np.log1p(y_train)\n",
    "        )  # Apply log1p transformation to the target variable during training\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_log = regressor.predict(X_test)\n",
    "        y_pred_train_log = regressor.predict(X_train)\n",
    "\n",
    "        # Inverse transform predictions to get back the original scale\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        y_pred_train = np.expm1(y_pred_train_log)\n",
    "\n",
    "        # Check for NaN or infinity values in y_pred or y_test\n",
    "        if (\n",
    "            np.isnan(y_pred).any()\n",
    "            or np.isinf(y_pred).any()\n",
    "            or np.isnan(y_test).any()\n",
    "            or np.isinf(y_test).any()\n",
    "        ):\n",
    "            print(\n",
    "                f\"Warning: NaN or infinity values found in predictions or true values. Imputing 0 for problematic values in y_pred for fold {fold}.\"\n",
    "            )\n",
    "            y_pred[np.isnan(y_pred) | np.isinf(y_pred)] = 0\n",
    "            # Optionally, you can also handle y_test in a similar way if needed\n",
    "            # y_test[np.isnan(y_test) | np.isinf(y_test)] = 0\n",
    "\n",
    "        # Calculate RMSE on the original scale\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        rmse_scores.append((rmse, fold))\n",
    "\n",
    "        # Record results for 'date', 'Actual', 'Predicted', 'Fold', and 'Set' in a list\n",
    "        fold_predictions = list(\n",
    "            zip(\n",
    "                test_data[\"date\"],\n",
    "                y_test,\n",
    "                y_pred,\n",
    "                [fold] * len(test_data),\n",
    "                [\"test\"] * len(test_data),\n",
    "            )\n",
    "        )\n",
    "        fold_predictions += list(\n",
    "            zip(\n",
    "                train_data[\"date\"],\n",
    "                y_train,\n",
    "                y_pred_train,\n",
    "                [fold] * len(train_data),\n",
    "                [\"train\"] * len(train_data),\n",
    "            )\n",
    "        )\n",
    "        all_predictions.extend(fold_predictions)\n",
    "\n",
    "        # Calculate the size of each train-test split\n",
    "        num_records.append((len(train_data), len(test_data)))\n",
    "\n",
    "        return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5481d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_rmse_std(tuple_list):\n",
    "    \"\"\"\n",
    "    Computes the standard deviation of the root mean squared errors\n",
    "\n",
    "    @param tuple_list: List of RMSE scores for each split.\n",
    "    @return tuple of mean and standard deviation\n",
    "    \"\"\"\n",
    "    first_elements = [t[0] for t in tuple_list]\n",
    "    mean = np.mean(first_elements)\n",
    "    std = np.std(first_elements)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rmse_and_dates(model_rmse, model_split_dates, num_records, model_name):\n",
    "    \"\"\"\n",
    "    Prints the RMSE for each of the train test splits\n",
    "\n",
    "    @param model_rmse: List of RMSE scores for each split.\n",
    "    @param model_split_dates: List of (min_date, max_date) tuples for each split.\n",
    "    @param num_records: List of (train_size, test_size) tuples for each split.\n",
    "    @model_name: a string indicating the name of the model\n",
    "\n",
    "    @return: none\n",
    "    \"\"\"\n",
    "    # Print RMSE scores and split dates for each split\n",
    "    for i, (rmse, dates, records) in enumerate(\n",
    "        zip(model_rmse, model_split_dates, num_records)\n",
    "    ):\n",
    "        min_date, max_date = dates\n",
    "        num_train_records, num_test_records = records\n",
    "\n",
    "        min_date = min_date.date()\n",
    "        max_date = max_date.date()\n",
    "\n",
    "        print(\n",
    "            f\"Split {i + 1}: Min Date: {min_date}, Max Date: {max_date}, RMSE: {rmse[0]}, Train Records: {num_train_records}, Test Records: {num_test_records}\"\n",
    "        )\n",
    "\n",
    "    rmse_std = compute_rmse_std(model_rmse)\n",
    "    print(model_name, \"RMSE score: {:.4f} ({:.4f})\\n\".format(rmse_std[0], rmse_std[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116c396",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model(buoy, target):\n",
    "    \"\"\"\n",
    "    Train the model for linear regression and random forest\n",
    "    Parameters:\n",
    "    - buoy: the data we're training on\n",
    "    - target: str the thing we're trying to predict (either\n",
    "    wave_height or average_period)\n",
    "\n",
    "    Returns:\n",
    "    - lr_w_int_preds_df: linear regression with intercept results containing the columns: \"date\", \"Actual\", \"Predicted\", \"Fold\", \"Set\"\n",
    "    - rf_preds_df: random forest with intercept results containing the columns: \"date\", \"Actual\", \"Predicted\", \"Fold\", \"Set\"\n",
    "    - two_week_predictions_linear: the two week forecast for linear regression\n",
    "    - two_week_predictions_rf: the two week forecast for random forest\n",
    "    \"\"\"\n",
    "\n",
    "    ##################### LINEAR REGRESSION MODELS ###############\n",
    "    # 3 types here: with intercept, without intercept, and Elastic Net (both L1 and L2 regularization)\n",
    "    lr_w_int = LinearRegression()\n",
    "    lr_no_int = LinearRegression(fit_intercept=False)\n",
    "    elastic_net = ElasticNet(\n",
    "        alpha=0.01, l1_ratio=0.1\n",
    "    )  # Adjust alpha and l1_ratio as needed\n",
    "    (\n",
    "        lr_w_int_preds_df,\n",
    "        lr_w_int_rmse,\n",
    "        lr_w_int_split_dates,\n",
    "        num_records,\n",
    "        two_week_predictions_linear,\n",
    "    ) = time_series_split_regression(\n",
    "        buoy,\n",
    "        regressor=lr_w_int,\n",
    "        target_column=target\n",
    "    )\n",
    "\n",
    "    ###################### RANDOM FOREST MODEL ##################\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "    rf_preds_df, rf_rmse, rf_split_dates, num_records, two_week_predictions_rf = time_series_split_regression(\n",
    "        buoy,\n",
    "        regressor=rf,\n",
    "    )\n",
    "\n",
    "    return lr_w_int_preds_df, rf_preds_df, two_week_predictions_linear, two_week_predictions_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4e8c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def display_results(predictions):\n",
    "    \"\"\"\n",
    "    Show a line plot with the trained data, real data, and predictions\n",
    "    @param: predictions, a dataframe of predictions\n",
    "    @return: none\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(predictions[\"date\"], predictions['Predicted'], color='green', label = 'Predicted Wave Height')\n",
    "    plt.plot(predictions[\"date\"], predictions['Actual'], color = 'red', label = 'Real Wave Height')\n",
    "    plt.title('Wave Height Prediction (Linear Reg)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Wave Height')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('lin_reg.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60714355",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def filter_fold(data, fold_num):\n",
    "    \"\"\"\n",
    "    Filter the dataframe by fold number (ex: reduce the dataframe to frame 0 only)\n",
    "    @data: the dataframe containing the predictions\n",
    "    @fold_num: an int indicating the fold number\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_df = data[data['Fold'] == fold_num]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5ca5d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_rmse(merged_linear, merged_rf):\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared error of the linear regression\n",
    "    predictions and random forest predictions\n",
    "\n",
    "    Parameters:\n",
    "    merged_linear:\n",
    "    merged_rf:\n",
    "\n",
    "    target : either \"wave_height\" or \"average_period\". Variable we want to traiun and predict on.\n",
    "    \"\"\"\n",
    "\n",
    "    rmse_linear = rmse(merged_linear[\"wave_height\"].tolist(), merged_linear[\"prediction\"].tolist())\n",
    "    rmse_rf = rmse(merged_rf[\"wave_height\"].tolist(), merged_rf[\"prediction\"].tolist())\n",
    "\n",
    "    return rmse_linear, rmse_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84162d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def predict(f, c, target, data, buoy_interpolated):\n",
    "    \"\"\"\n",
    "    Returns predictions using random forest regression and linear regression.\n",
    "    This function is for testing the accuracy of our model. It lags the data \n",
    "    so we can calculate the daily error for the past two weeks.\n",
    "\n",
    "    Parameters:\n",
    "    - f: the number of days we used to train (ex: 45 days)\n",
    "    - c: the number of days we are forecasting (ex: 14 days)\n",
    "    - target: the thing we're trying to predict (average period or wave height)\n",
    "    - data: dataframe of not cleaned data\n",
    "    - buoy_interpolated: dataframe of cleaned data\n",
    "\n",
    "    Returns:\n",
    "    - merged_linear: dataframe of linear regression predictions and the recent 14 days of data (predictions and actual data)\n",
    "    - merged_rf: dataframe of random forest predictions and the recent 14 days of data\n",
    "    \"\"\"\n",
    "    # Sets up date objects and floor and ceiling\n",
    "    today_date = datetime.today().date()\n",
    "    floor = f + c\n",
    "    ceiling = c\n",
    "\n",
    "    # Calculate the floordate and the ceiling date\n",
    "    floorDate = today_date - timedelta(days=floor)\n",
    "    ceilingDate = today_date - timedelta(days=ceiling)\n",
    "\n",
    "    # Splits up the df into recent and past\n",
    "    # Recent holds the 14 most recent days of data\n",
    "    # Past holds all data from the floor to the ceiling\n",
    "    recent_df = data[data['date'] > pd.Timestamp(ceilingDate)]\n",
    "    past_df = buoy_interpolated[(buoy_interpolated['date'] > pd.Timestamp(floorDate)) & (buoy_interpolated['date'] < pd.Timestamp(ceilingDate))]\n",
    "\n",
    "    lr_w_int_preds_df, rf_preds_df, two_week_predictions_linear, two_week_predictions_rf = train_model(past_df, target)\n",
    "\n",
    "    # LINEAR PREDICTIONS   \n",
    "    start_date = datetime.now()\n",
    "    end_date = start_date + timedelta(days=14)  # Extend two weeks\n",
    "    prediction_dates = pd.date_range(start=start_date, end=end_date, freq='D').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Convert prediction_dates to a DataFrame\n",
    "    linear_prediction = pd.DataFrame(prediction_dates, columns=['date'])\n",
    "    num_predictions = len(linear_prediction)\n",
    "    \n",
    "    # Merge prediction_dates_df and two_week_predictions_linear_df\n",
    "    linear_prediction['prediction'] = two_week_predictions_linear[-num_predictions:]\n",
    "\n",
    "    # RANDOM FOREST PREDICTIONS\n",
    "    # Convert prediction_dates to a DataFrame\n",
    "    rf_predictions = pd.DataFrame(prediction_dates, columns=['date'])\n",
    "\n",
    "    # Merge prediction_dates_df and two_week_predictions_linear_df\n",
    "    rf_predictions['prediction'] = two_week_predictions_rf[-num_predictions:]\n",
    "\n",
    "    # Assuming 'date' column in recent_df is not datetime type\n",
    "    recent_df['date'] = pd.to_datetime(recent_df['date'])\n",
    "\n",
    "    # Assuming 'date' column in linear_prediction is not datetime type\n",
    "    linear_prediction['date'] = pd.to_datetime(linear_prediction['date'])\n",
    "    rf_predictions['date'] = pd.to_datetime(rf_predictions['date'])\n",
    "    \n",
    "    # Merge linear_prediction and recent_df using merge_asof\n",
    "    merged_linear = pd.merge_asof(linear_prediction, recent_df, on='date', direction='nearest')\n",
    "\n",
    "    # Merge rf_predictions and recent_df using merge_asof\n",
    "    merged_rf = pd.merge_asof(rf_predictions, recent_df, on='date', direction='nearest')\n",
    "\n",
    "    return merged_linear, merged_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280870eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Root Mean Squared Error (RMSE).\n",
    "    \n",
    "    Parameters:\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "        \n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Estimated target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The RMSE value.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred)) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354002a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def to_table(future):\n",
    "    \"\"\"\n",
    "    Create a table of predictions with corresponding day and time\n",
    "\n",
    "    @param future: the dataframe of predicted and actual values for the future\n",
    "    @return table of predictions with corresponding day and time\n",
    "    \"\"\"\n",
    "    table = []\n",
    "    \n",
    "    for index, row in future.iterrows():\n",
    "        day = datetime.strptime(str(row['date']), \"%Y-%m-%d %H:%M:%S\").strftime(\"%A %B %d\")\n",
    "        predicted = str(round(row['prediction'],2))\n",
    "        \n",
    "        table.append([day, predicted])\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_std(predictions):\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation of the predictions\n",
    "\n",
    "    @param predictions: the array of predictions we have\n",
    "    @return standard_dev: the standard deviation of the predictions\n",
    "    \"\"\"\n",
    "    standard_dev = predictions.std()\n",
    "    return standard_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_std(target, prophet_std, linear_std, rf_std):\n",
    "    \"\"\"\n",
    "    Graphs the standard deviations of the predictions for\n",
    "    all three algorithms for either wave height or wave period\n",
    "\n",
    "    @param target: the target we're predicting for (average period or wave height)\n",
    "    @param prophet_std: Prophet algorithm's standard deviation\n",
    "    @param linear_std: the linear standard deviation\n",
    "    @param rf_std: random forest standard deviation\n",
    "\n",
    "    @return: none\n",
    "    \"\"\"\n",
    "    # Plotting the bars\n",
    "    plt.bar(['Prophet', 'Linear Regression', 'Random Forest'], [prophet_std, linear_std, rf_std])\n",
    "\n",
    "    if target == \"wave_height\":\n",
    "        plt.title('Standard Deviation of Wave Height Predictions')\n",
    "    elif target == \"average_period\":\n",
    "        plt.title('Standard Deviation of Average Period Predictions')\n",
    "\n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453df5c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def graph_daily_error(merged_linear, merged_rf, prediction_type, df):\n",
    "    \"\"\"\n",
    "    Uses the calculated daily error from daily_error() and displays it in graph form\n",
    "\n",
    "    @param: merged_linear\n",
    "    @param: prediction_type (wave height or average period)\n",
    "\n",
    "    @return: none\n",
    "    \"\"\"\n",
    "    if prediction_type == \"wave_height\":\n",
    "        daily_error_linear = daily_error(merged_linear[\"wave_height\"].tolist(), merged_linear[\"prediction\"].tolist())\n",
    "        daily_error_rf = daily_error(merged_rf[\"wave_height\"].tolist(), merged_rf[\"prediction\"].tolist())\n",
    "    elif prediction_type == \"average_period\":\n",
    "        daily_error_linear = daily_error(merged_linear[\"average_period\"].tolist(), merged_linear[\"prediction\"].tolist())\n",
    "        daily_error_rf = daily_error(merged_rf[\"average_period\"].tolist(), merged_rf[\"prediction\"].tolist())       \n",
    "\n",
    "    plt.plot(merged_linear.index, daily_error_linear[\"daily_error\"], label='Linear Regression')\n",
    "    \n",
    "    plt.plot(merged_rf.index, daily_error_rf[\"daily_error\"], label='Random Forest')\n",
    "\n",
    "    # add Prophet's daily error\n",
    "    df[\"difference\"] = df['difference'] = df['yhat'] - df[f'{prediction_type}_interpolated']\n",
    "    df['difference'] = df['difference'].abs()\n",
    "\n",
    "    plt.plot(df.reset_index().index, df['difference'], label='Prophet')\n",
    "    \n",
    "    if prediction_type == \"wave_height\":\n",
    "        plt.title('Daily Error Comparison Wave Height')\n",
    "    elif prediction_type == \"average_period\":\n",
    "        plt.title('Daily Error Comparison Average Period')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Error (meters for wave height and seconds for average period)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb079f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prophet(prediction_type, buoy_num):\n",
    "    \"\"\"\n",
    "    Calls the prophet model with our given params. \n",
    "    rse_per_day() comes from the prophet_rse script.\n",
    "\n",
    "    @param prediction_type: the target variable we're predicting for\n",
    "    @param buoy_num: buoy number to predict for\n",
    "\n",
    "    @return: the dataframe of predicted values using prophet\n",
    "    \"\"\"\n",
    "    df = rse_per_day(720, 15, prediction_type, buoy_num)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Daily Error.\n",
    "\n",
    "    Parameters:\n",
    "        y_true : array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "        \n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Estimated target values.\n",
    "\n",
    "    Returns:\n",
    "        dataframe of y_true, y_pred, and daily error\n",
    "    \"\"\"\n",
    "    # Check if lengths of y_true and y_pred are equal\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Lengths of y_true and y_pred must be equal.\")\n",
    "\n",
    "    # Calculate residuals\n",
    "    daily_error = abs(np.array(y_true) - np.array(y_pred))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'daily_error': daily_error})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408be269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Predict wave_height and average_period 2 weeks in advance\n",
    "    \"\"\"\n",
    "\n",
    "    variable_list = ['wave_height', 'average_period']\n",
    "\n",
    "    # buoys that are compatible: 44091, 44089, 44100, 44086, 41117, 42036,\n",
    "    # 46232, 46047, 46219, 46251, 46221, 46268, 46222, 46253, 46224, 46275,\n",
    "    # 46277, 46256, 46274, 46225, 46266, 46014, 46013, 46214, 46026, 46237,\n",
    "    # 46239, 46011, 46218, 46054, 46053\n",
    "    buoy_num = \"46011\"\n",
    "\n",
    "    # # prompting user to enter a target variable\n",
    "    target_variable_choice = 0\n",
    "    while target_variable_choice != 1 and target_variable_choice != 2:\n",
    "        print(\"1 - Wave Height \\n2 - Wave Period\")\n",
    "        target_variable_choice = int(input(\"Enter prediction choice: \"))\n",
    "\n",
    "\n",
    "    buoy = gather_data(buoy_num) \n",
    "\n",
    "    buoy = buoy.reset_index()\n",
    "    target_variable = variable_list[target_variable_choice - 1]\n",
    "\n",
    "    buoy_cleaned = handle_missing_data(buoy)\n",
    "\n",
    "    period_linear, period_rf = predict(45, 14, \"average_period\", buoy, buoy_cleaned)\n",
    "    rmse_linear, rmse_rf = calculate_rmse(period_linear, period_rf)\n",
    "    print(f\"LINEAR AVERAGE PERIOD RMSE: {rmse_linear} \\nRF AVERAGE PERIOD RMSE: {rmse_rf}\\n\")\n",
    "    table_period = to_table(period_linear) \n",
    "\n",
    "    height_linear, height_rf = predict(45, 14, \"wave_height\", buoy, buoy_cleaned)\n",
    "    rmse_linear, rmse_rf = calculate_rmse(height_linear, height_rf)\n",
    "    print(f\"LINEAR WAVE HEIGHT RMSE: {rmse_linear} \\nRF WAVE HEIGHT RMSE: {rmse_rf}\\n\")\n",
    "    table_height = to_table(height_linear) \n",
    "\n",
    "    ###################### GRAPHING ERROR ######################\n",
    "    prophet_wave_height = predict_prophet('wave_height', buoy_num)\n",
    "    prophet_avg_period = predict_prophet('average_period', buoy_num)\n",
    "    graph_daily_error(height_linear, height_rf, 'wave_height', prophet_wave_height)\n",
    "    graph_daily_error(period_linear, period_rf, 'average_period', prophet_avg_period) \n",
    "\n",
    "    ###################### GRAPHING STANDARD DEVIATION ######################\n",
    "\n",
    "    # wave height std dev\n",
    "    linear_std = calculate_std(height_linear['prediction'])\n",
    "    rf_std = calculate_std(height_rf['prediction'])\n",
    "    prophet_std = calculate_std(prophet_wave_height['yhat'])\n",
    "    graph_std('wave_height', prophet_std, linear_std, rf_std)\n",
    "\n",
    "    # avg period std dev\n",
    "    linear_std = calculate_std(period_linear['prediction'])\n",
    "    rf_std = calculate_std(period_rf['prediction'])\n",
    "    prophet_std = calculate_std(prophet_avg_period['yhat'])\n",
    "    graph_std('average_period', prophet_std, linear_std, rf_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547395b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
